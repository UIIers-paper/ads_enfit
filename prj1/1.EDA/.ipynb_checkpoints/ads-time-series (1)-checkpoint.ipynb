{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'folium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfolium\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'folium'"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import folium\n",
    "\n",
    "\n",
    "import warnings\n",
    "# Filter out specific ValueWarnings from statsmodels\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: folium in c:\\users\\pc\\miniconda3\\envs\\data\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: branca>=0.6.0 in c:\\users\\pc\\miniconda3\\envs\\data\\lib\\site-packages (from folium) (0.7.2)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\pc\\miniconda3\\envs\\data\\lib\\site-packages (from folium) (3.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\pc\\miniconda3\\envs\\data\\lib\\site-packages (from folium) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\miniconda3\\envs\\data\\lib\\site-packages (from folium) (2.32.2)\n",
      "Requirement already satisfied: xyzservices in c:\\users\\pc\\miniconda3\\envs\\data\\lib\\site-packages (from folium) (2024.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pc\\miniconda3\\envs\\data\\lib\\site-packages (from jinja2>=2.9->folium) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\miniconda3\\envs\\data\\lib\\site-packages (from requests->folium) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\miniconda3\\envs\\data\\lib\\site-packages (from requests->folium) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\miniconda3\\envs\\data\\lib\\site-packages (from requests->folium) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\miniconda3\\envs\\data\\lib\\site-packages (from requests->folium) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
    "\n",
    "## Define important columns\n",
    "data_cols = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id','prediction_unit_id']\n",
    "client_cols = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
    "gas_prices_cols = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh','origin_date','data_block_id']\n",
    "electricity_prices_cols = ['forecast_date', 'euros_per_mwh','origin_date','data_block_id']\n",
    "forecast_weather_cols = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
    "historical_weather_cols = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure','cloudcover_total','cloudcover_low','cloudcover_mid','cloudcover_high','windspeed_10m','winddirection_10m','shortwave_radiation','direct_solar_radiation','diffuse_radiation','latitude','longitude']\n",
    "location_cols = ['longitude', 'latitude', 'county']\n",
    "target_cols = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime']\n",
    "\n",
    "## Importing only specified columns\n",
    "df_data = pl.read_csv(os.path.join(root, \"train.csv\"), columns=data_cols, try_parse_dates=True)\n",
    "df_client = pl.read_csv(os.path.join(root, \"client.csv\"), columns=client_cols, try_parse_dates=True)\n",
    "df_gas_prices = pl.read_csv(os.path.join(root, \"gas_prices.csv\"), columns=gas_prices_cols, try_parse_dates=True)\n",
    "df_electricity_prices = pl.read_csv(os.path.join(root, \"electricity_prices.csv\"), columns=electricity_prices_cols, try_parse_dates=True)\n",
    "df_forecast_weather = pl.read_csv(os.path.join(root, \"forecast_weather.csv\"), columns=forecast_weather_cols, try_parse_dates=True)\n",
    "df_historical_weather = pl.read_csv(os.path.join(root, \"historical_weather.csv\"), columns=historical_weather_cols, try_parse_dates=True)\n",
    "df_weather_station_to_county_mapping = pl.read_csv(os.path.join(root, \"weather_station_to_county_mapping.csv\"), columns=location_cols, try_parse_dates=True)\n",
    "df_target = df_data.select(target_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use ERD to display data information for each table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M√¥ t·∫£ B·ªô D·ªØ Li·ªáu\n",
    "Th·ª≠ th√°ch c·ªßa b·∫°n trong cu·ªôc thi n√†y l√† d·ª± ƒëo√°n l∆∞·ª£ng ƒëi·ªán ƒë∆∞·ª£c s·∫£n xu·∫•t v√† ti√™u th·ª• b·ªüi c√°c kh√°ch h√†ng nƒÉng l∆∞·ª£ng ·ªü Estonia ƒë√£ l·∫Øp ƒë·∫∑t c√°c t·∫•m pin m·∫∑t tr·ªùi. B·∫°n s·∫Ω c√≥ quy·ªÅn truy c·∫≠p v√†o d·ªØ li·ªáu th·ªùi ti·∫øt, gi√° nƒÉng l∆∞·ª£ng li√™n quan v√† h·ªì s∆° v·ªÅ c√¥ng su·∫•t quang ƒëi·ªán ƒë∆∞·ª£c l·∫Øp ƒë·∫∑t.\n",
    "\n",
    "ƒê√¢y l√† cu·ªôc thi d·ª± b√°o s·ª≠ d·ª•ng API chu·ªói th·ªùi gian. B·∫£ng x·∫øp h·∫°ng ri√™ng t∆∞ s·∫Ω ƒë∆∞·ª£c x√°c ƒë·ªãnh b·∫±ng d·ªØ li·ªáu th·ª±c t·∫ø thu th·∫≠p sau khi k·∫øt th√∫c th·ªùi gian n·ªôp b√†i.\n",
    "\n",
    "üí° L∆∞u √Ω:\n",
    "T·∫•t c·∫£ c√°c b·ªô d·ªØ li·ªáu tu√¢n theo c√πng m·ªôt quy ∆∞·ªõc th·ªùi gian. Th·ªùi gian ƒë∆∞·ª£c cung c·∫•p theo EET/EEST. H·∫ßu h·∫øt c√°c bi·∫øn l√† t·ªïng ho·∫∑c trung b√¨nh trong kho·∫£ng th·ªùi gian 1 gi·ªù. C·ªôt datetime (d√π t√™n l√† g√¨) lu√¥n cho bi·∫øt th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu c·ªßa kho·∫£ng th·ªùi gian 1 gi·ªù. Tuy nhi√™n, ƒë·ªëi v·ªõi c√°c b·ªô d·ªØ li·ªáu th·ªùi ti·∫øt, m·ªôt s·ªë bi·∫øn nh∆∞ nhi·ªát ƒë·ªô ho·∫∑c ƒë·ªô che ph·ªß m√¢y ƒë∆∞·ª£c cung c·∫•p cho m·ªôt th·ªùi ƒëi·ªÉm c·ª• th·ªÉ, lu√¥n l√† th·ªùi ƒëi·ªÉm k·∫øt th√∫c c·ªßa kho·∫£ng th·ªùi gian 1 gi·ªù.\n",
    "\n",
    "C√°c t·ªáp\n",
    "train.csv\n",
    "\n",
    "- county: M√£ ID cho huy·ªán.\n",
    "- is_business: Boolean cho bi·∫øt li·ªáu ng∆∞·ªùi ti√™u th·ª• c√≥ ph·∫£i l√† doanh nghi·ªáp hay kh√¥ng.\n",
    "- product_type: M√£ ID v·ªõi √°nh x·∫° sau ƒë√¢y c·ªßa c√°c m√£ t·ªõi c√°c lo·∫°i h·ª£p ƒë·ªìng: {0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}.\n",
    "- target: L∆∞·ª£ng ti√™u th·ª• ho·∫∑c s·∫£n xu·∫•t cho ph√¢n ƒëo·∫°n li√™n quan trong gi·ªù. C√°c ph√¢n ƒëo·∫°n ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi county, is_business, v√† product_type.\n",
    "- is_consumption: Boolean cho bi·∫øt li·ªáu m·ª•c ti√™u c·ªßa h√†ng n√†y l√† ti√™u th·ª• hay s·∫£n xu·∫•t.\n",
    "- datetime: Th·ªùi gian Estonia theo EET (UTC+2) / EEST (UTC+3). N√≥ m√¥ t·∫£ th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu c·ªßa kho·∫£ng th·ªùi gian 1 gi·ªù m√† m·ª•c ti√™u ƒë∆∞·ª£c cung c·∫•p.\n",
    "- data_block_id: T·∫•t c·∫£ c√°c h√†ng chia s·∫ª c√πng m·ªôt data_block_id s·∫Ω c√≥ s·∫µn t·∫°i c√πng th·ªùi ƒëi·ªÉm d·ª± b√°o. ƒê√¢y l√† ch·ª©c nƒÉng c·ªßa nh·ªØng th√¥ng tin c√≥ s·∫µn khi c√°c d·ª± b√°o th·ª±c s·ª± ƒë∆∞·ª£c th·ª±c hi·ªán, v√†o l√∫c 11 gi·ªù s√°ng m·ªói ng√†y. V√≠ d·ª•, n·∫øu d·ª± b√°o th·ªùi ti·∫øt data_block_id cho c√°c d·ª± b√°o ƒë∆∞·ª£c th·ª±c hi·ªán v√†o ng√†y 31 th√°ng 10 l√† 100 th√¨ data_block_id th·ªùi ti·∫øt l·ªãch s·ª≠ cho ng√†y 31 th√°ng 10 s·∫Ω l√† 101 v√¨ d·ªØ li·ªáu th·ªùi ti·∫øt l·ªãch s·ª≠ ch·ªâ th·ª±c s·ª± c√≥ s·∫µn v√†o ng√†y h√¥m sau.\n",
    "- row_id: M·ªôt m√£ ƒë·ªãnh danh duy nh·∫•t cho h√†ng.\n",
    "- prediction_unit_id: M·ªôt m√£ ƒë·ªãnh danh duy nh·∫•t cho s·ª± k·∫øt h·ª£p gi·ªØa county, is_business, v√† product_type. C√°c ƒë∆°n v·ªã d·ª± b√°o m·ªõi c√≥ th·ªÉ xu·∫•t hi·ªán ho·∫∑c bi·∫øn m·∫•t trong t·∫≠p ki·ªÉm tra.\n",
    "\n",
    "gas_prices.csv\n",
    "\n",
    "- origin_date: Ng√†y khi gi√° tr∆∞·ªõc m·ªôt ng√†y c√≥ s·∫µn.\n",
    "- forecast_date: Ng√†y khi gi√° d·ª± b√°o c√≥ li√™n quan.\n",
    "- [lowest/highest]_price_per_mwh: Gi√° th·∫•p nh·∫•t/cao nh·∫•t c·ªßa kh√≠ ƒë·ªët t·ª± nhi√™n tr√™n th·ªã tr∆∞·ªùng tr∆∞·ªõc m·ªôt ng√†y v√†o ng√†y giao d·ªãch ƒë√≥, t√≠nh b·∫±ng Euro m·ªói megawatt gi·ªù t∆∞∆°ng ƒë∆∞∆°ng.\n",
    "- data_block_id\n",
    "\n",
    "client.csv\n",
    "\n",
    "- product_type\n",
    "- county: M√£ ID cho huy·ªán. Xem county_id_to_name_map.json ƒë·ªÉ √°nh x·∫° m√£ ID t·ªõi t√™n huy·ªán.\n",
    "- eic_count: S·ªë ƒëi·ªÉm ti√™u th·ª• t·ªïng h·ª£p (EICs - M√£ ƒë·ªãnh danh Ch√¢u √Çu).\n",
    "- installed_capacity: C√¥ng su·∫•t t·∫•m pin m·∫∑t tr·ªùi quang ƒëi·ªán ƒë∆∞·ª£c l·∫Øp ƒë·∫∑t t√≠nh b·∫±ng kilowatt.\n",
    "- is_business: Boolean cho bi·∫øt li·ªáu ng∆∞·ªùi ti√™u th·ª• c√≥ ph·∫£i l√† doanh nghi·ªáp hay kh√¥ng.\n",
    "- date\n",
    "- data_block_id\n",
    "\n",
    "electricity_prices.csv\n",
    "\n",
    "- origin_date\n",
    "- forecast_date: ƒê·∫°i di·ªán cho th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu c·ªßa kho·∫£ng th·ªùi gian 1 gi·ªù khi gi√° c√≥ hi·ªáu l·ª±c.\n",
    "- euros_per_mwh: Gi√° ƒëi·ªán tr√™n c√°c th·ªã tr∆∞·ªùng tr∆∞·ªõc m·ªôt ng√†y t√≠nh b·∫±ng Euro m·ªói megawatt gi·ªù.\n",
    "- data_block_id\n",
    "\n",
    "forecast_weather.csv\n",
    "\n",
    "D·ª± b√°o th·ªùi ti·∫øt c√≥ s·∫µn v√†o th·ªùi ƒëi·ªÉm d·ª± b√°o. Ngu·ªìn t·ª´ Trung t√¢m D·ª± b√°o Th·ªùi ti·∫øt T·∫ßm trung Ch√¢u √Çu.\n",
    "\n",
    "- [latitude/longitude]: T·ªça ƒë·ªô c·ªßa d·ª± b√°o th·ªùi ti·∫øt.\n",
    "- origin_datetime: D·∫•u th·ªùi gian c·ªßa khi d·ª± b√°o ƒë∆∞·ª£c t·∫°o ra.\n",
    "- hours_ahead: S·ªë gi·ªù gi·ªØa th·ªùi ƒëi·ªÉm t·∫°o d·ª± b√°o v√† th·ªùi ƒëi·ªÉm d·ª± b√°o th·ªùi ti·∫øt. M·ªói d·ª± b√°o bao g·ªìm t·ªïng c·ªông 48 gi·ªù.\n",
    "- temperature: Nhi·ªát ƒë·ªô kh√¥ng kh√≠ ·ªü 2 m√©t tr√™n m·∫∑t ƒë·∫•t t√≠nh b·∫±ng ƒë·ªô C. ƒê∆∞·ª£c ∆∞·ªõc t√≠nh cho th·ªùi ƒëi·ªÉm k·∫øt th√∫c c·ªßa kho·∫£ng th·ªùi gian 1 gi·ªù.\n",
    "- dewpoint: Nhi·ªát ƒë·ªô ƒëi·ªÉm s∆∞∆°ng ·ªü 2 m√©t tr√™n m·∫∑t ƒë·∫•t t√≠nh b·∫±ng ƒë·ªô C. ƒê∆∞·ª£c ∆∞·ªõc t√≠nh cho th·ªùi ƒëi·ªÉm k·∫øt th√∫c c·ªßa kho·∫£ng th·ªùi gian 1 gi·ªù.\n",
    "- cloudcover_[low/mid/high/total]: T·ª∑ l·ªá ph·∫ßn trƒÉm c·ªßa b·∫ßu tr·ªùi ƒë∆∞·ª£c che ph·ªß b·ªüi m√¢y ·ªü c√°c d·∫£i ƒë·ªô cao sau: 0-2 km, 2-6 km, 6+ km, v√† t·ªïng c·ªông. ƒê∆∞·ª£c ∆∞·ªõc t√≠nh cho th·ªùi ƒëi·ªÉm k·∫øt th√∫c c·ªßa kho·∫£ng th·ªùi gian 1 gi·ªù.\n",
    "- 10_metre_[u/v]_wind_component: Th√†nh ph·∫ßn [h∆∞·ªõng ƒë√¥ng/h∆∞·ªõng b·∫Øc] c·ªßa t·ªëc ƒë·ªô gi√≥ ƒëo ·ªü 10 m√©t tr√™n b·ªÅ m·∫∑t t√≠nh b·∫±ng m√©t tr√™n gi√¢y. ƒê∆∞·ª£c ∆∞·ªõc t√≠nh cho th·ªùi ƒëi·ªÉm k·∫øt th√∫c c·ªßa kho·∫£ng th·ªùi gian 1 gi·ªù.\n",
    "- data_block_id\n",
    "- forecast_datetime: D·∫•u th·ªùi gian c·ªßa th·ªùi ti·∫øt d·ª± b√°o. ƒê∆∞·ª£c t·∫°o t·ª´ origin_datetime c·ªông v·ªõi hours_ahead. ƒêi·ªÅu n√†y ƒë·∫°i di·ªán cho th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu c·ªßa kho·∫£ng th·ªùi gian 1 gi·ªù m√† d·ªØ li·ªáu th·ªùi ti·∫øt ƒë∆∞·ª£c d·ª± b√°o.\n",
    "- direct_solar_radiation: B·ª©c x·∫° m·∫∑t tr·ªùi tr·ª±c ti·∫øp chi·∫øu xu·ªëng b·ªÅ m·∫∑t tr√™n m·ªôt m·∫∑t ph·∫≥ng vu√¥ng g√≥c v·ªõi h∆∞·ªõng c·ªßa m·∫∑t tr·ªùi t√≠ch l≈©y trong gi·ªù, t√≠nh b·∫±ng watt-gi·ªù tr√™n m√©t vu√¥ng.\n",
    "- surface_solar_radiation_downwards: B·ª©c x·∫° m·∫∑t tr·ªùi, bao g·ªìm c·∫£ tr·ª±c ti·∫øp v√† khu·∫øch t√°n, chi·∫øu xu·ªëng m·ªôt m·∫∑t ph·∫≥ng ngang tr√™n b·ªÅ m·∫∑t Tr√°i ƒê·∫•t, t√≠ch l≈©y trong gi·ªù, t√≠nh b·∫±ng watt-gi·ªù tr√™n m√©t vu√¥ng.\n",
    "- snowfall: L∆∞·ª£ng tuy·∫øt r∆°i trong gi·ªù t√≠nh b·∫±ng m√©t n∆∞·ªõc t∆∞∆°ng ƒë∆∞∆°ng.\n",
    "- total_precipitation: L∆∞·ª£ng m∆∞a t√≠ch l≈©y, bao g·ªìm c·∫£ m∆∞a v√† tuy·∫øt r∆°i tr√™n b·ªÅ m·∫∑t Tr√°i ƒê·∫•t trong gi·ªù ƒë∆∞·ª£c m√¥ t·∫£, t√≠nh b·∫±ng m√©t.\n",
    "\n",
    "historical_weather.csv\n",
    "\n",
    "D·ªØ li·ªáu th·ªùi ti·∫øt l·ªãch s·ª≠.\n",
    "\n",
    "- datetime: ƒê·∫°i di·ªán cho th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu c·ªßa kho·∫£ng th·ªùi gian 1 gi·ªù m√† d·ªØ li·ªáu th·ªùi ti·∫øt ƒë∆∞·ª£c ƒëo.\n",
    "- temperature: ƒê∆∞·ª£c ƒëo ·ªü th·ªùi ƒëi·ªÉm k·∫øt th√∫c c·ªßa kho·∫£ng th·ªùi gian 1 gi·ªù.\n",
    "- dewpoint: ƒê∆∞·ª£c ƒëo ·ªü th·ªùi ƒëi·ªÉm k·∫øt th√∫c c·ªßa kho·∫£ng th·ªùi gian 1 gi·ªù.\n",
    "- rain: Kh√°c v·ªõi c√°c quy ∆∞·ªõc d·ª± b√°o. L∆∞·ª£ng m∆∞a t·ª´ c√°c h·ªá th·ªëng th·ªùi ti·∫øt l·ªõn trong gi·ªù t√≠nh b·∫±ng milimet.\n",
    "- snowfall: Kh√°c v·ªõi c√°c quy ∆∞·ªõc d·ª± b√°o. L∆∞·ª£ng tuy·∫øt r∆°i trong gi·ªù t√≠nh b·∫±ng centimet.\n",
    "- surface_pressure: √Åp su·∫•t kh√¥ng kh√≠ ·ªü m·∫∑t ƒë·∫•t t√≠nh b·∫±ng hectopascal.\n",
    "- cloudcover_[low/mid/high/total]: Kh√°c v·ªõi c√°c quy ∆∞·ªõc d·ª± b√°o. ƒê·ªô che ph·ªß m√¢y ·ªü c√°c ƒë·ªô cao 0-3 km, 3-8 km, 8+, v√† t·ªïng c·ªông.\n",
    "- windspeed_10m: Kh√°c v·ªõi c√°c quy ∆∞·ªõc d·ª± b√°o. T·ªëc ƒë·ªô gi√≥ ·ªü 10 m√©t tr√™n m·∫∑t ƒë·∫•t t√≠nh b·∫±ng m√©t tr√™n gi√¢y.\n",
    "- winddirection_10m: Kh√°c v·ªõi c√°c quy ∆∞·ªõc d·ª± b√°o. H∆∞·ªõng gi√≥ ·ªü 10 m√©t tr√™n m·∫∑t ƒë·∫•t t√≠nh b·∫±ng ƒë·ªô.\n",
    "- shortwave_radiation: Kh√°c v·ªõi c√°c quy ∆∞·ªõc d·ª± b√°o. B·ª©c x·∫° to√†n c·∫ßu tr√™n m·∫∑t ph·∫≥ng ngang t√≠nh b·∫±ng watt-gi·ªù tr√™n m√©t vu√¥ng.\n",
    "- direct_solar_radiation\n",
    "- diffuse_radiation: Kh√°c v·ªõi c√°c quy ∆∞·ªõc d·ª± b√°o. B·ª©c x·∫° khu·∫øch t√°n t√≠nh b·∫±ng watt-gi·ªù tr√™n m√©t vu√¥ng.\n",
    "- [latitude/longitude]: T·ªça ƒë·ªô c·ªßa tr·∫°m th·ªùi ti·∫øt.\n",
    "- data_block_id\n",
    "\n",
    "public_timeseries_testing_util.py\n",
    "\n",
    "M·ªôt t·ªáp t√πy ch·ªçn nh·∫±m gi√∫p d·ªÖ d√†ng h∆°n khi ch·∫°y c√°c b√†i ki·ªÉm tra API t√πy ch·ªânh offline. Xem docstring c·ªßa script ƒë·ªÉ bi·∫øt chi ti·∫øt. B·∫°n s·∫Ω c·∫ßn ch·ªânh s·ª≠a t·ªáp n√†y tr∆∞·ªõc khi s·ª≠ d·ª•ng.\n",
    "\n",
    "example_test_files/\n",
    "\n",
    "D·ªØ li·ªáu nh·∫±m minh h·ªça c√°ch API ho·∫°t ƒë·ªông. Bao g·ªìm c√°c t·ªáp v√† c·ªôt t∆∞∆°ng t·ª± ƒë∆∞·ª£c cung c·∫•p b·ªüi API. Ba data_block_id ƒë·∫ßu ti√™n l√† s·ª± l·∫∑p l·∫°i c·ªßa ba data_block_id cu·ªëi c√πng trong t·∫≠p hu·∫•n luy·ªán.\n",
    "\n",
    "example_test_files/sample_submission.csv\n",
    "\n",
    "M·ªôt m·∫´u n·ªôp h·ª£p l·ªá, ƒë∆∞·ª£c cung c·∫•p b·ªüi API. Xem notebook n√†y ƒë·ªÉ bi·∫øt m·ªôt v√≠ d·ª• r·∫•t ƒë∆°n gi·∫£n v·ªÅ c√°ch s·ª≠ d·ª•ng m·∫´u n·ªôp.\n",
    "\n",
    "example_test_files/revealed_targets.csv\n",
    "\n",
    "C√°c gi√° tr·ªã m·ª•c ti√™u th·ª±c t·∫ø t·ª´ ng√†y tr∆∞·ªõc th·ªùi gian d·ª± b√°o. ƒêi·ªÅu n√†y t∆∞∆°ng ƒë∆∞∆°ng v·ªõi ƒë·ªô tr·ªÖ hai ng√†y so v·ªõi th·ªùi gian d·ª± b√°o trong test.csv.\n",
    "\n",
    "enefit/\n",
    "\n",
    "C√°c t·ªáp cho ph√©p API ho·∫°t ƒë·ªông. D·ª± ki·∫øn API s·∫Ω cung c·∫•p t·∫•t c·∫£ c√°c h√†ng trong v√≤ng d∆∞·ªõi 15 ph√∫t v√† ch·ªâ s·ª≠ d·ª•ng √≠t h∆°n 0,5\n",
    "\n",
    " GB b·ªô nh·ªõ. Phi√™n b·∫£n API m√† b·∫°n c√≥ th·ªÉ t·∫£i xu·ªëng cung c·∫•p d·ªØ li·ªáu t·ª´ example_test_files/. B·∫°n ph·∫£i ƒë∆∞a ra d·ª± b√°o cho nh·ªØng ng√†y ƒë√≥ ƒë·ªÉ ti·∫øn h√†nh API nh∆∞ng nh·ªØng d·ª± b√°o ƒë√≥ s·∫Ω kh√¥ng ƒë∆∞·ª£c ch·∫•m ƒëi·ªÉm. D·ª± ki·∫øn c√≥ kho·∫£ng ba th√°ng d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p ban ƒë·∫ßu v√† t·ªëi ƒëa m∆∞·ªùi th√°ng d·ªØ li·ªáu v√†o cu·ªëi k·ª≥ d·ª± b√°o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from IPython.display import Image\n",
    "import os\n",
    "\n",
    "# Create Digraph object\n",
    "dot = Digraph(comment='ERD')\n",
    "\n",
    "# Define tables and columns along with primary and foreign keys\n",
    "tables = {\n",
    "    'df_target': {\n",
    "        'columns': df_target.columns,\n",
    "        'primary_key': 'datetime',\n",
    "        'foreign_keys': {}  # Add foreign_keys if any\n",
    "    },\n",
    "    'df_gas_prices': {\n",
    "        'columns': df_gas_prices.columns,\n",
    "        'primary_key': 'forecast_date ',\n",
    "        'foreign_keys': {}  # Add foreign_keys if any\n",
    "    },\n",
    "    'df_electricity_prices': {\n",
    "        'columns': df_electricity_prices.columns,\n",
    "        'primary_key': 'forecast_date ',\n",
    "        'foreign_keys': {}  # Add foreign_keys if any\n",
    "    },\n",
    "    'df_forecast_weather': {\n",
    "        'columns': df_forecast_weather.columns,\n",
    "        'primary_key': 'datetime',\n",
    "        'foreign_keys': {}  # Add foreign_keys if any\n",
    "    },\n",
    "    'df_historical_weather': {\n",
    "        'columns': df_historical_weather.columns,\n",
    "        'primary_key': 'datetime',\n",
    "        'foreign_keys': {}  # Add foreign_keys if any\n",
    "    },\n",
    "    'df_weather_station_to_county_mapping': {\n",
    "        'columns': df_weather_station_to_county_mapping.columns,\n",
    "        'primary_key': 'county',  # Assuming station_id is the primary key\n",
    "        'foreign_keys': {}  # Add foreign_keys if any\n",
    "    },\n",
    "    'df_client': {\n",
    "        'columns': df_client.columns,\n",
    "        'primary_key': 'date',  # Assuming client_id is the primary key\n",
    "        'foreign_keys': {}  # Add foreign_keys if any\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add tables to the graph\n",
    "for table, details in tables.items():\n",
    "    columns = details['columns']\n",
    "    primary_key = details['primary_key']\n",
    "    foreign_keys = details['foreign_keys']\n",
    "    \n",
    "    # Create labels for columns\n",
    "    column_labels = []\n",
    "    for column in columns:\n",
    "        if column == primary_key:\n",
    "            column_labels.append(f'<{column}> {column} (PK)')\n",
    "        elif column in foreign_keys:\n",
    "            column_labels.append(f'<{column}> {column} (FK)')\n",
    "        else:\n",
    "            column_labels.append(f'<{column}> {column}')\n",
    "    \n",
    "    # Add table to the graph\n",
    "    label = f'{{{table} | {\"|\".join(column_labels)}}}'\n",
    "    dot.node(table, label=label, shape='record')\n",
    "\n",
    "# Add foreign key relationships to the graph\n",
    "for table, details in tables.items():\n",
    "    foreign_keys = details['foreign_keys']\n",
    "    for fk, ref_table in foreign_keys.items():\n",
    "        dot.edge(f'{table}:{fk}', f'{ref_table}:{fk}')\n",
    "\n",
    "# Render and save the graph as a PNG file\n",
    "output_path = 'ERD'\n",
    "dot.render(output_path, format='png')\n",
    "\n",
    "# Display the image\n",
    "display_image_path = output_path + '.png'\n",
    "if os.path.exists(display_image_path):\n",
    "    display(Image(display_image_path))\n",
    "else:\n",
    "    print(\"Error: The image file was not created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Descriptive Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data size: {len(df_data )}\")\n",
    "\n",
    "display(df_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gas size: {len(df_gas_prices )}\")\n",
    "\n",
    "display(df_gas_prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gas_prices.to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Electric size: {len(df_electricity_prices )}\")\n",
    "\n",
    "display(df_electricity_prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_electricity_prices.to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Client size: {len(df_client )}\")\n",
    "\n",
    "display(df_client.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client.to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"forecast size: {len(df_forecast_weather )}\")\n",
    "\n",
    "display(df_forecast_weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_weather.to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_forecast_weather.to_pandas().hours_ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"histirical size: {len(df_historical_weather )}\")\n",
    "\n",
    "display(df_historical_weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_historical_weather.to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Weather station size: {len(df_weather_station_to_county_mapping )}\")\n",
    "print(type(df_weather_station_to_county_mapping))\n",
    "\n",
    "df_weather_station_to_county_mapping = df_weather_station_to_county_mapping.drop_nulls()\n",
    "display(df_weather_station_to_county_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Weather station size: {len(df_weather_station_to_county_mapping )}\")\n",
    "\n",
    "display(df_weather_station_to_county_mapping.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_station_to_county_mapping.to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Weather station size: {len(df_target )}\")\n",
    "\n",
    "display(df_target.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data is followed from '{min(df_target['datetime'])}' to '{max(df_target['datetime'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os. path.join(root, f\"county_id_to_name_map.json\")) as fo:\n",
    "    county_id_to_name = json.load(fo)\n",
    "print(county_id_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_locations = {\n",
    "    \"HARJUMAA\": (59.351142, 24.725384),\n",
    "    \"HIIUMAA\": (58.918082, 22.586403),\n",
    "    \"IDA-VIRUMAA\": (59.228971, 27.406654),\n",
    "    \"J√ÑRVAMAA\": (58.897934, 25.623048),\n",
    "    \"J√ïGEVAMAA\": (58.727941, 26.413961),\n",
    "    \"L√Ñ√ÑNE-VIRUMAA\": (59.267897, 26.363968),\n",
    "    \"L√Ñ√ÑNEMAA\": (58.975935, 23.772451),\n",
    "    \"P√ÑRNUMAA\": (58.448793, 24.526469),\n",
    "    \"P√ïLVAMAA\": (58.089925, 27.101149),\n",
    "    \"RAPLAMAA\": (58.924451, 24.619842),\n",
    "    \"SAAREMAA\": (58.414075, 22.525137),\n",
    "    \"TARTUMAA\": (58.394168, 26.747568),\n",
    "    \"VALGAMAA\": (57.933466, 26.191360),\n",
    "    \"VILJANDIMAA\": (58.316916, 25.595130),\n",
    "    \"V√ïRUMAA\": (57.765485, 27.025669)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_loc = [\"latitude\", \"longitude\"]\n",
    "weather_locations = df_historical_weather.to_pandas().groupby(cols_loc).size().reset_index()[cols_loc]\n",
    "display(weather_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transform and pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df_data, df_client, df_gas_prices, df_electricity_prices, df_forecast_weather, df_historical_weather, df_weather_station_to_county_mapping, df_target):\n",
    "    df_data = (\n",
    "        df_data.with_columns(pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),)\n",
    "    )\n",
    "    \n",
    "    df_gas_prices = (\n",
    "        df_gas_prices.rename({\"forecast_date\": \"date\"})\n",
    "    )\n",
    "    \n",
    "    df_electricity_prices = (\n",
    "        df_electricity_prices.rename({\"forecast_date\": \"datetime\"})\n",
    "    )\n",
    "    \n",
    "    df_weather_station_to_county_mapping = (\n",
    "        df_weather_station_to_county_mapping.with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32))\n",
    "    )\n",
    "    \n",
    "    # sum of all product_type targets related to [\"datetime\", \"county\", \"is_business\", \"is_consumption\"]\n",
    "    df_target_all_type_sum = (\n",
    "        df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\")\n",
    "    )\n",
    "    \n",
    "    df_forecast_weather = (\n",
    "        df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"}).filter(pl.col(\"hours_ahead\") >= 24) # we don't need forecast for today\n",
    "        .with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            # datetime for forecast in a different timezone\n",
    "            pl.col('datetime').dt.replace_time_zone(None).cast(pl.Datetime(\"us\")),\n",
    "        )\n",
    "        .join(df_weather_station_to_county_mapping, how=\"left\", on=[\"longitude\", \"latitude\"]).drop(\"longitude\", \"latitude\")\n",
    "    )\n",
    "    \n",
    "    df_historical_weather = (\n",
    "        df_historical_weather\n",
    "        .with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "        )\n",
    "        .join(df_weather_station_to_county_mapping, how=\"left\", on=[\"longitude\", \"latitude\"]).drop(\"longitude\", \"latitude\")\n",
    "    )\n",
    "    \n",
    "    # creating average forecast characteristics for all weather stations\n",
    "    df_forecast_weather_date = (\n",
    "        df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "    )\n",
    "    \n",
    "    # creating average forecast characteristics for weather stations related to county\n",
    "    df_forecast_weather_local = (\n",
    "        df_forecast_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean()\n",
    "    )\n",
    "    \n",
    "    # creating average historical characteristics for all weather stations\n",
    "    df_historical_weather_date = (\n",
    "        df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "    )\n",
    "    \n",
    "    # creating average historical characteristics for weather stations related to county\n",
    "    df_historical_weather_local = (\n",
    "        df_historical_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean()\n",
    "    )\n",
    "    \n",
    "    df_data = (\n",
    "        df_data\n",
    "        # pl.duration(days=1) shifts datetime to join lag features (usually we join last available values)\n",
    "        .join(df_gas_prices.with_columns((pl.col(\"date\") + pl.duration(days=1)).cast(pl.Date)), on=\"date\", how=\"left\")\n",
    "        .join(df_client.with_columns((pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)), on=[\"county\", \"is_business\", \"product_type\", \"date\"], how=\"left\")\n",
    "        .join(df_electricity_prices.with_columns(pl.col(\"datetime\") + pl.duration(days=1)), on=\"datetime\", how=\"left\")\n",
    "        \n",
    "        # lag forecast_weather features (24 hours * days)\n",
    "        .join(df_forecast_weather_date, on=\"datetime\", how=\"left\", suffix=\"_fd\")\n",
    "        .join(df_forecast_weather_local, on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_fl\")\n",
    "        .join(df_forecast_weather_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_fd_7d\")\n",
    "        .join(df_forecast_weather_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_fl_7d\")\n",
    "\n",
    "        # lag historical_weather features (24 hours * days)\n",
    "        .join(df_historical_weather_date.with_columns(pl.col(\"datetime\") + pl.duration(days=2)), on=\"datetime\", how=\"left\", suffix=\"_hd_2d\")\n",
    "        .join(df_historical_weather_local.with_columns(pl.col(\"datetime\") + pl.duration(days=2)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hl_2d\")\n",
    "        .join(df_historical_weather_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_hd_7d\")\n",
    "        .join(df_historical_weather_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hl_7d\")\n",
    "        \n",
    "        # lag target features (24 hours * days)\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=2)).rename({\"target\": \"target_1\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=3)).rename({\"target\": \"target_2\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=4)).rename({\"target\": \"target_3\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=5)).rename({\"target\": \"target_4\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=6)).rename({\"target\": \"target_5\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=7)).rename({\"target\": \"target_6\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=14)).rename({\"target\": \"target_7\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        \n",
    "        .join(df_target_all_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(days=2)).rename({\"target\": \"target_1\"}), on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"], suffix=\"_all_type_sum\", how=\"left\")\n",
    "        .join(df_target_all_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(days=3)).rename({\"target\": \"target_2\"}), on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"], suffix=\"_all_type_sum\", how=\"left\")\n",
    "        .join(df_target_all_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(days=7)).rename({\"target\": \"target_6\"}), on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"], suffix=\"_all_type_sum\", how=\"left\")\n",
    "        .join(df_target_all_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(days=14)).rename({\"target\": \"target_7\"}), on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"], suffix=\"_all_type_sum\", how=\"left\")\n",
    "        \n",
    "        \n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "            pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "            pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "            pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "            pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "            pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.concat_str(\"county\", \"is_business\", \"product_type\", \"is_consumption\", separator=\"_\").alias(\"segment\"),\n",
    "        )\n",
    "        # cyclical features encoding https://towardsdatascience.com/cyclical-features-encoding-its-about-time-ce23581845ca\n",
    "        .with_columns(\n",
    "            (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
    "            (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "            (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "            (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(pl.Float64).cast(pl.Float32),\n",
    "        )\n",
    "        \n",
    "        .drop(\"datetime\", \"hour\", \"dayofyear\")\n",
    "    )\n",
    "    \n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function defined for merging dataframes with the row_id as unique identifier\n",
    "def to_pandas(X, y=None):\n",
    "    cat_cols = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"segment\"]\n",
    "    \n",
    "    if y is not None:\n",
    "        df = pd.concat([X.to_pandas(), y.to_pandas()], axis=1)\n",
    "    else:\n",
    "        df = X.to_pandas()    \n",
    "    \n",
    "    df = df.set_index(\"row_id\")\n",
    "    df[cat_cols] = df[cat_cols].astype(\"category\")\n",
    "    \n",
    "    df[\"target_mean\"] = df[[f\"target_{i}\" for i in range(1, 7)]].mean(1)\n",
    "    df[\"target_std\"] = df[[f\"target_{i}\" for i in range(1, 7)]].std(1)\n",
    "    df[\"target_ratio\"] = df[\"target_6\"] / (df[\"target_7\"] + 1e-3)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Preprocessing\n",
    "\n",
    "1. **Splitting Data into Features and Target**\n",
    "   - `df_data` is updated to exclude the 'target' column.\n",
    "   - `y` is created to contain only the 'target' column from `df_data`.\n",
    "\n",
    "2. **Generating Additional Features**\n",
    "   - The `generate_features` function is used to enrich `df_data` with new features created from related datasets like client, gas prices, electricity prices, forecast and historical weather data, and weather station mappings.\n",
    "\n",
    "3. **Transforming Data for Pandas DataFrame**\n",
    "   - The `to_pandas` function is applied to transform the enriched data into a pandas DataFrame, integrating the target values and creating additional engineered features such as 'target_mean', 'target_std', and 'target_ratio'.\n",
    "\n",
    "4. **Cleaning Data**\n",
    "   - Rows with null values in the 'target' column are removed to ensure data quality.\n",
    "   - The dataset is filtered to include only records from the year 2022 and beyond.\n",
    "\n",
    "The processed dataset, `df_train_features`, is now optimized and ready for use in machine learning tasks, free from null target values and focused on recent data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data, y = df_data.drop(\"target\"), df_data.select(\"target\")\n",
    "\n",
    "county_id_to_name = {\n",
    "    0: 'HARJUMAA', 1: 'HIIUMAA', 2: 'IDA-VIRUMAA', 3: 'J√ÑRVAMAA', 4: 'J√ïGEVAMAA', \n",
    "    5: 'L√Ñ√ÑNE-VIRUMAA', 6: 'L√ÑNEMAA', 7: 'P√ÑRNUMAA', 8: 'P√ïLVAMAA', 9: 'RAPLAMAA', \n",
    "    10: 'SAAREMAA', 11: 'TARTUMAA', 12: 'UNKNOWN', 13: 'VALGAMAA', 14: 'VILJANDIMAA', 15: 'V√ïRUMAA'\n",
    "}\n",
    "\n",
    "# Chuy·ªÉn c·ªôt 'county' th√†nh ki·ªÉu chu·ªói\n",
    "# df_train_features['county'] = df_train_features['county'].astype(str)\n",
    "\n",
    "# T·∫°o c·ªôt m·ªõi 'county_name' v√† √°nh x·∫° gi√° tr·ªã c·ªßa c·ªôt 'county' t·ªõi t√™n qu·∫≠n t∆∞∆°ng ·ª©ng\n",
    "\n",
    "df_train_features = generate_features(df_data, df_client, df_gas_prices, df_electricity_prices, df_forecast_weather, df_historical_weather, df_weather_station_to_county_mapping, df_target)\n",
    "\n",
    "df_train_features = to_pandas(df_train_features, y)\n",
    "# a little proportion of target values are null\n",
    "df_train_features = df_train_features[df_train_features['target'].notnull()]\n",
    "# df_train_features['country'] = df_train_features['county'].map(county_id_to_name)\n",
    "\n",
    "\n",
    "# filter old data\n",
    "# df_train_features = df_train_features[df_train_features.year >= 2022]\n",
    "\n",
    "df_train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_train_features.segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EDA and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_ = folium.Map(location=[58.595272, 25.013607], zoom_start=7)\n",
    "\n",
    "for county, coords in counties_locations.items():\n",
    "    folium.CircleMarker(\n",
    "        location=[coords[0], coords[1]], radius=5, color='cornflowerblue', fill=True\n",
    "    ).add_child(folium.Popup(county)).add_to(map_)\n",
    "\n",
    "for _, loc in weather_locations.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[loc[\"latitude\"], loc[\"longitude\"]], radius=1, color='orange'\n",
    "    ).add_to(map_)\n",
    "    \n",
    "map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chu·∫©n b·ªã d·ªØ li·ªáu\n",
    "non_business_counts = df_train_features[df_train_features['is_business'] == 1].groupby('county').size()\n",
    "business_counts = df_train_features[df_train_features['is_business'] == 0].groupby('county').size()\n",
    "\n",
    "# T·∫°o bi·ªÉu ƒë·ªì c·ªôt ch·ªìng\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "bar_width = 0.7\n",
    "\n",
    "# V·∫Ω c·ªôt 'business' ch·ªìng l√™n 'non_business'\n",
    "bar1 = ax.bar(range(len(business_counts)), business_counts, bar_width, label='Business')\n",
    "\n",
    "# V·∫Ω c·ªôt 'non_business'\n",
    "bar2 = ax.bar(range(len(non_business_counts)), non_business_counts, bar_width,bottom=business_counts.values, label='Non Business')\n",
    "\n",
    "\n",
    "\n",
    "# ƒê·∫∑t nh√£n v√† ti√™u ƒë·ªÅ\n",
    "ax.set_xlabel('County', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Prosumer Distribution by County', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Thi·∫øt l·∫≠p nh√£n x nghi√™ng 45 ƒë·ªô\n",
    "ax.set_xticks(range(len(non_business_counts)))\n",
    "ax.set_xticklabels(non_business_counts.index.map(county_id_to_name.get), rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "# Th√™m ch√∫ th√≠ch\n",
    "ax.legend(title='Is Business', loc='upper right')\n",
    "\n",
    "# Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_dict = df_train_features[\"county\"].value_counts().to_dict()\n",
    "county_df = pd.DataFrame({\"County\":county_dict.keys(), \"Count\":county_dict.values()})\n",
    "# print(county_df)\n",
    "px.bar(county_df, x=\"County\", y=\"Count\", title=\"County Distribution\", color=\"County\")\n",
    "# ax.set_xticklabels(non_business_counts.index.map(county_id_to_name.get), rotation=45, ha='right', fontsize=10)\n",
    "# Assuming county_df['County'] contains county IDs that need to be mapped to names\n",
    "county_df['County_Name'] = county_df['County'].map(county_id_to_name)\n",
    "\n",
    "# Generate the bar plot with updated county names\n",
    "fig = px.bar(county_df, x=\"County_Name\", y=\"Count\", title=\"County Distribution\", color=\"Count\")\n",
    "\n",
    "# Optional: Update the layout to rotate the x-axis tick labels\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_business_dict = df_train_features[\"is_business\"].value_counts().to_dict()\n",
    "is_business_df = pd.DataFrame({\"Business\":is_business_dict.keys(), \"Count\":is_business_dict.values()})\n",
    "fig=px.bar(is_business_df, x=\"Business\", y=\"Count\", title=\"Business Distribution\", color=\"Business\")\n",
    "# Update the layout to hide the x-axis labels\n",
    "fig.update_layout(\n",
    "    xaxis={'showticklabels': False},\n",
    "    legend_title_text='Business'\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prosumer_type_counts = df_train_features['is_business'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.set_palette(\"deep\")\n",
    "plt.pie(prosumer_type_counts, labels=prosumer_type_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.legend(title='Prosumer Type', loc='upper right')\n",
    "plt.title('Prosumer Type Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_prices_over_time = px.line(df_gas_prices, x='forecast_date', y=['lowest_price_per_mwh', 'highest_price_per_mwh'],\n",
    "                               title='Gas Prices Over Time on Forecast Date ',\n",
    "                               labels={'forecast_date': 'Forecast Date', 'value': 'Price (Euro/MWh)'})\n",
    "gas_prices_over_time.update_layout(legend_title_text='Price Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_prices_over_time = px.line(df_gas_prices, x='origin_date', y=['lowest_price_per_mwh', 'highest_price_per_mwh'],\n",
    "                               title='Gas Prices Over Time on Origin Date',\n",
    "                               labels={'origin_date': 'Origin Date', 'value': 'Price (Euro/MWh)'})\n",
    "gas_prices_over_time.update_layout(legend_title_text='Price Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_prices_over_time = px.line(df_gas_prices, x='data_block_id', y=['lowest_price_per_mwh', 'highest_price_per_mwh'],\n",
    "                               title='Data Block Id based on Gas Prices Comparision',\n",
    "                               labels={'data_block_id': 'Block ID', 'value': 'Price (Euro/MWh)'})\n",
    "gas_prices_over_time.update_layout(legend_title_text='Price Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming df_electricity_prices is your DataFrame\n",
    "# Check and convert forecast_date to string if it's not already\n",
    "if df_electricity_prices['forecast_date'].dtype != pl.Utf8:\n",
    "    df_electricity_prices = df_electricity_prices.with_columns(\n",
    "        pl.col('forecast_date').cast(pl.Utf8)\n",
    "    )\n",
    "\n",
    "# Convert forecast_date to datetime\n",
    "df_electricity_prices = df_electricity_prices.with_columns(\n",
    "    pl.col('forecast_date').str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S%.f\")\n",
    ")\n",
    "\n",
    "# Extract date part from datetime\n",
    "df_electricity_prices = df_electricity_prices.with_columns(\n",
    "    pl.col('forecast_date').dt.date().alias('date')\n",
    ")\n",
    "\n",
    "# Group by date and calculate the mean of euros_per_mwh\n",
    "df_electricity_prices_daily = df_electricity_prices.groupby('date').agg(\n",
    "    pl.col('euros_per_mwh').mean().alias('euros_per_mwh')\n",
    ").sort('date')\n",
    "\n",
    "# Convert Polars DataFrame to Pandas DataFrame for Plotly\n",
    "df_electricity_prices_pandas = df_electricity_prices.to_pandas()\n",
    "df_electricity_prices_daily_pandas = df_electricity_prices_daily.to_pandas()\n",
    "\n",
    "# Create traces\n",
    "trace1 = go.Scatter(\n",
    "    x=df_electricity_prices_pandas['forecast_date'],\n",
    "    y=df_electricity_prices_pandas['euros_per_mwh'],\n",
    "    mode='lines',\n",
    "    name='Electricity Prices',\n",
    "    line=dict(color='gray')\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x=df_electricity_prices_daily_pandas['date'],\n",
    "    y=df_electricity_prices_daily_pandas['euros_per_mwh'],\n",
    "    mode='lines',\n",
    "    name='Daily Average',\n",
    "    line=dict(color='orange')\n",
    ")\n",
    "\n",
    "# Create the figure and add traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(trace1)\n",
    "fig.add_trace(trace2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Electricity Prices',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Euros per MWh',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_electricity_prices, x=\"origin_date\", y=\"euros_per_mwh\", title = \"Electricity Price Distribution Based on Forecast Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_electricity_prices, x=\"forecast_date\", y=\"euros_per_mwh\", title = \"Electricity Price Distribution Based on Forecast Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dewpoint_comparison = px.scatter(df_forecast_weather[:1000], x='forecast_datetime', y='temperature',\n",
    "                                      color='dewpoint', title='Temperature vs. Dewpoint Over Time',\n",
    "                                      labels={'forecast_datetime': 'Forecast Datetime', 'temperature': 'Temperature (¬∞C)', 'dewpoint': 'Dewpoint (¬∞C)'})\n",
    "temp_dewpoint_comparison.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_speed_solar_radiation = px.scatter(df_forecast_weather[:1000], x='forecast_datetime', y='10_metre_u_wind_component',\n",
    "                                        color='surface_solar_radiation_downwards',\n",
    "                                        title='Wind Speed vs. Solar Radiation Over Time',\n",
    "                                        labels={'forecast_datetime': 'Forecast Datetime', '10_metre_u_wind_component': 'Wind Speed (m/s)', 'surface_solar_radiation_downwards': 'Solar Radiation (W/m^2)'})\n",
    "wind_speed_solar_radiation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Filter the dataset for the specific segment\n",
    "consumption_0_0_1_1 = df_train_features[df_train_features.segment == '0_0_1_1']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='date', y='target', data=consumption_0_0_1_1, color='blue', linewidth=2)\n",
    "\n",
    "plt.title('Target Over Time for Segment 0_0_1_1', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Target', fontsize=12)\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # Format the date display\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=15))  # Adjust the interval for better readability\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for clarity\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "c=['orangered', 'blue', 'gold', 'red', 'green', 'purple', 'orange', 'brown','gray','white','pink','yellow','silver','violet']\n",
    "segment_list = df_train_features.segment.unique()[:len(c)]\n",
    "\n",
    "i=0\n",
    "# Filter the dataset for the specific segment\n",
    "for seg in segment_list:\n",
    "    consumption_segment = df_train_features[df_train_features.segment == seg]\n",
    "\n",
    "    # Create a line plot using Plotly Express\n",
    "    fig = px.line(consumption_segment, x='date', y='target', \n",
    "              title=f'Target Over Time for Segment {seg}',\n",
    "              labels={'date': 'Date', 'target': 'Target'},\n",
    "              template='plotly_dark',line_shape='linear')\n",
    "    fig.update_traces(line=dict(color=c[i], width=1.5))\n",
    "\n",
    "    # Customize the x-axis date format and tick interval\n",
    "    fig.update_xaxes(type='date', tickformat='%Y-%m-%d', tickmode='linear', dtick=15)\n",
    "    i+=1\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    \n",
    "    prediction_unit_id = i\n",
    "\n",
    "    df_plot = df_train_features[\n",
    "    (df_train_features['prediction_unit_id'] == prediction_unit_id)\n",
    "    ]\n",
    "\n",
    "    fig = px.line(\n",
    "      df_plot,\n",
    "      x='date',\n",
    "      y='target',\n",
    "      color='is_consumption',\n",
    "      title=f'Target for Prediction Unit: {prediction_unit_id}'\n",
    ")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analyze base on Time series Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFT Analysis of Segments\n",
    "\n",
    "To uncover potential high-frequency patterns within the dataset that may not be immediately apparent, a Fast Fourier Transform (FFT) analysis is conducted on the first ten unique segments of the dataset. This analysis aims to reveal hidden frequency-based characteristics and periodicities within these segments.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - The first ten unique segments from `df_train_features` are selected for analysis.\n",
    "   - A subset of the dataset, `example_df`, is created, containing only the data from these segments.\n",
    "\n",
    "2. **FFT Computation**:\n",
    "   - FFT is applied to the 'target' column of each segment. This transformation converts the time-domain data into the frequency domain, enabling the identification of dominant frequencies in the data.\n",
    "\n",
    "3. **Frequency Analysis**:\n",
    "   - The frequencies corresponding to each FFT value are calculated.\n",
    "   - The magnitude of each FFT value is computed to quantify the strength of each frequency component.\n",
    "\n",
    "4. **Normalization and Filtering**:\n",
    "   - The magnitudes are normalized by dividing them by the maximum magnitude in each segment, ensuring comparability across segments.\n",
    "   - Frequencies longer than the 'Semiannual' period are filtered out to focus on more relevant, shorter-term cycles.\n",
    "\n",
    "5. **Visualization**:\n",
    "   - A log-scaled plot is created to visualize the frequency spectra of each segment.\n",
    "   - Each segment's spectrum is offset vertically for clarity, allowing distinct visualization of each segment's frequency characteristics.\n",
    "\n",
    "### Conclusions\n",
    "- In the first few segments analyzed, there is a notable presence of weak weekly periodicity.\n",
    "- Across all segments, patterns with daily and hourly frequencies are prominently observed. These distinct patterns could be pivotal in identifying features that are correlated with or causally related to weather conditions or energy prices. Alternatively, they may well be artifacts caused by how the data are reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_list = df_train_features.segment.unique()[:12]\n",
    "example_df = df_train_features[np.isin(df_train_features.segment,segment_list)]\n",
    "\n",
    "# Periods now in hours\n",
    "periods_in_hours = {\n",
    "    'Quarterly': (365 / 4) * 24,\n",
    "    'Monthly': 30 * 24,\n",
    "    'Weekly': 7 * 24,\n",
    "    'Daily': 24,\n",
    "    '12-hour': 12,\n",
    "    '8-hour': 8,\n",
    "    '6-hour': 6,\n",
    "    '4-hour': 4\n",
    "}\n",
    "frequencies_for_periods = {k: 1/v for k, v in periods_in_hours.items()}\n",
    "\n",
    "# Plot adjustments remain mostly the same\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.xscale('log')\n",
    "\n",
    "for i, segment in enumerate(example_df['segment'].unique()):\n",
    "    segment_data = example_df[example_df['segment'] == segment]['target']\n",
    "    fft_values = np.fft.fft(segment_data)\n",
    "    frequencies = np.fft.fftfreq(len(fft_values), d=1)  # d=1 because data is hourly\n",
    "    magnitudes = np.abs(fft_values)[frequencies > 0]\n",
    "    normalized_magnitudes = magnitudes / np.max(magnitudes)\n",
    "    positive_freqs = frequencies[frequencies > 0]\n",
    "\n",
    "    # Adjust the filtering based on updated frequencies for periods\n",
    "    valid_freqs = positive_freqs[positive_freqs > frequencies_for_periods['Quarterly']]\n",
    "    valid_magnitudes = normalized_magnitudes[positive_freqs > frequencies_for_periods['Quarterly']]\n",
    "\n",
    "    offset_magnitudes = valid_magnitudes + i  # Offset for clarity\n",
    "\n",
    "    plt.plot(valid_freqs, offset_magnitudes, label=f'Segment {segment}')\n",
    "\n",
    "plt.title('Frequency Spectra of hourly target for Each Segment')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Normalized Magnitude + Offset')\n",
    "plt.xticks(list(frequencies_for_periods.values()), list(frequencies_for_periods.keys()),rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_plots_enefit(name):\n",
    "    # Initialize the figure for the spectrum using Plotly\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Convert the x-axis to a log scale\n",
    "    fig.update_xaxes(type='log')\n",
    "\n",
    "    # Plot the spectrum for the specified segment\n",
    "    segment_data = example_df[example_df['segment'] == '0_0_1_1'][name]\n",
    "    fft_values = np.fft.fft(segment_data)\n",
    "    frequencies = np.fft.fftfreq(len(fft_values), d=1)\n",
    "    magnitudes = np.abs(fft_values)[frequencies > 0]\n",
    "    positive_freqs = frequencies[frequencies > 0]\n",
    "\n",
    "    # Filter out frequencies corresponding to periods longer than 'Semiannual'\n",
    "    valid_freqs = positive_freqs[positive_freqs > frequencies_for_periods['Quarterly']]\n",
    "    valid_magnitudes = magnitudes[positive_freqs > frequencies_for_periods['Quarterly']]\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=valid_freqs, y=valid_magnitudes, mode='lines', name='0_0_1_1'))\n",
    "\n",
    "    #  Customize the plot layout\n",
    "    fig.update_layout(\n",
    "    title=f'{name} frequency spectrum',\n",
    "    xaxis_title='Frequency',\n",
    "    yaxis_title='Magnitude',\n",
    "    xaxis=dict(tickvals=list(frequencies_for_periods.values()), ticktext=list(frequencies_for_periods.keys())),\n",
    "    showlegend=True,\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_list=['temperature','direct_solar_radiation','hours_ahead','highest_price_per_mwh']\n",
    "for i in plot_list:\n",
    "    fft_plots_enefit(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_list = df_train_features.segment.unique()[:10]\n",
    "example_df = df_train_features[np.isin(df_train_features.segment,segment_list)]\n",
    "\n",
    "# Initialize the figure for the spectra\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Convert the x-axis to a log scale\n",
    "plt.xscale('log')\n",
    "\n",
    "# Plot the spectrum for each segment with offset\n",
    "for i, segment in enumerate(example_df['segment'].unique()):\n",
    "    segment_data = example_df[example_df['segment'] == segment].groupby('date')['target'].mean()\n",
    "    fft_values = np.fft.fft(segment_data)\n",
    "    frequencies = np.fft.fftfreq(len(fft_values), d=1)\n",
    "    magnitudes = np.abs(fft_values)[frequencies > 0]\n",
    "    normalized_magnitudes = magnitudes / np.max(magnitudes)\n",
    "    positive_freqs = frequencies[frequencies > 0]\n",
    "\n",
    "    # Offset each segment's spectrum for clarity\n",
    "    offset_magnitudes = normalized_magnitudes + i\n",
    "\n",
    "    plt.plot(positive_freqs, offset_magnitudes, label=f'Segment {segment}')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Frequency Spectra of daily averaged target for Each Segment')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Normalized Magnitude + Offset')\n",
    "\n",
    "# Set the x-axis ticks and labels to the calculated frequencies for the periods\n",
    "plt.xticks(list(frequencies_for_periods.values()), list(frequencies_for_periods.keys()))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular-Spectrum Analysis (SSA)\n",
    "Another tool for time-series data analysis is Singular Spectrum Analysis (SSA), which can be considered similar to Principal Component Analysis (PCA) but specifically tailored for time-series data.\n",
    "\n",
    "This method involves constructing a trajectory matrix using lagged versions of the time series as its columns. By performing singular value decomposition (SVD) on this trajectory matrix, we can extract the principal components of the time series, which encapsulate key temporal characteristics such as trends and periodicities.\n",
    "\n",
    "Unlike the Fourier transform (fft), SSA does not restrict components to be periodic; it can also capture irregular components, such as trends, which fft may not effectively represent. For more detailed explanation into this method, refer to this excellent [notebook](https://www.kaggle.com/code/jdarcy/introducing-ssa-for-time-series-decomposition/notebook#4.-A-Python-Class-for-SSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd, matrix_rank\n",
    "\n",
    "class SSA:\n",
    "    def __init__(self, series, window_size):\n",
    "        self.series = np.array(series)\n",
    "        self.window_size = window_size\n",
    "        self.n = len(series)\n",
    "        self.trajectory_matrix = self.create_trajectory_matrix()\n",
    "\n",
    "    def create_trajectory_matrix(self):\n",
    "        n_vectors = self.n - self.window_size + 1\n",
    "        return np.column_stack([self.series[i:i+self.window_size] for i in range(n_vectors)])\n",
    "\n",
    "    def decompose(self):\n",
    "        U, sigma, Vt = svd(self.trajectory_matrix)\n",
    "        self.rank = matrix_rank(self.trajectory_matrix)\n",
    "        return U, sigma, Vt\n",
    "\n",
    "    def reconstruct(self, U, sigma, Vt, components):\n",
    "        rank = len(components)\n",
    "        if rank > self.rank:\n",
    "            raise ValueError(\"Number of components cannot be greater than the rank of the trajectory matrix\")\n",
    "\n",
    "        series_components = []\n",
    "        for i in components:\n",
    "            X_i = sigma[i] * np.outer(U[:,i], Vt[i,:])\n",
    "            series_components.append(self.diagonal_averaging(X_i))\n",
    "        \n",
    "        return sum(series_components)\n",
    "\n",
    "    def diagonal_averaging(self, X_i):\n",
    "        n_rows, n_cols = X_i.shape\n",
    "        diagonals = [X_i[::-1, :].diagonal(i) for i in range(-X_i.shape[0]+1, X_i.shape[1])]\n",
    "        return np.array([np.mean(diagonal) for diagonal in diagonals])\n",
    "    \n",
    "    def calculate_w_correlation(self):\n",
    "        n = self.window_size\n",
    "        N = self.n\n",
    "        K = N - n + 1\n",
    "        w = np.array([min(i+1, n, N-i) for i in range(N)])\n",
    "        w_corr_matrix = np.zeros((self.rank, self.rank))\n",
    "\n",
    "        for i in range(self.rank):\n",
    "            for j in range(self.rank):\n",
    "                if i <= j:\n",
    "                    F_i = self.reconstruct(U, sigma, Vt, [i])\n",
    "                    F_j = self.reconstruct(U, sigma, Vt, [j])\n",
    "                    w_corr_matrix[i, j] = np.sum(w * F_i * F_j) / np.sqrt(np.sum(w * F_i ** 2) * np.sum(w * F_j ** 2))\n",
    "                    w_corr_matrix[j, i] = w_corr_matrix[i, j]\n",
    "        \n",
    "        return w_corr_matrix\n",
    "\n",
    "    def plot_w_correlation(self, U, sigma, Vt):\n",
    "        w_corr_matrix = self.calculate_w_correlation()\n",
    "        plt.imshow(w_corr_matrix, cmap='hot', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.title('W-Correlation Matrix')\n",
    "        plt.xlabel('Component')\n",
    "        plt.ylabel('Component')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSA Reconstruction\n",
    "Considering the artifacts introduced by the hourly data, we will use daily aggregated data for our SSA analysis.\n",
    "\n",
    "When we reconstruct the time series using the first five components, it becomes apparent that these components effectively capture the general trends of the time series. Furthermore, the analysis of the residuals reveals critical insights. Apart from the annual trend captured by the first few components, it is essential to consider high-frequency components that also contribute significantly to the dataset's characteristics, such as those wave-like fluctuations at the beginning of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'date' and calculate the mean of 'target' for each day\n",
    "daily_avg = consumption_0_0_1_1.groupby('date')['target'].mean()\n",
    "\n",
    "# Decompose the time series into 200 components, set by window_size\n",
    "window_size = 200\n",
    "ssa = SSA(daily_avg, window_size)\n",
    "U, sigma, Vt = ssa.decompose()\n",
    "\n",
    "# Reconstruct the series using the first 10 components\n",
    "reconstructed_series = ssa.reconstruct(U, sigma, Vt, components=list(range(10)))\n",
    "\n",
    "# Plot the original and reconstructed series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_avg.index, daily_avg.values, alpha=0.5, label='Original Daily Average Series',color='blue')\n",
    "plt.plot(daily_avg.index, reconstructed_series, label='Reconstructed Series',linewidth=1.5,color='orange')\n",
    "plt.title('Original vs Reconstructed Series')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Target')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Subtract the reconstructed series from the original series\n",
    "residual_series = daily_avg - reconstructed_series\n",
    "\n",
    "# Plot the residual series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_avg.index, residual_series, label='Residual Series',color='blue')\n",
    "plt.title('Residual Series after Subtracting Reconstructed Components')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Residual Target Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSA Components\n",
    "The first ten mode shapes are presented below. The initial few components predominantly represent trends, while the higher components increasingly exhibit oscillatory behaviors, characterized by progressively higher frequencies. Notice that some modes are quite similar and that is totally normal in SSA. The resemblance between different modes can be visualized in the w-correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 20))\n",
    "for i in range(10):\n",
    "    plt.subplot(10, 1, i+1)\n",
    "    plt.plot(Vt[i, :], label=f'Component {i+1}',linewidth=1.5,color='blue')\n",
    "    plt.title(f'Mode Shape of Component {i+1}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSA Components\n",
    "The first ten mode shapes are presented below. The initial few components predominantly represent trends, while the higher components increasingly exhibit oscillatory behaviors, characterized by progressively higher frequencies. Notice that some modes are quite similar and that is totally normal in SSA. The resemblance between different modes can be visualized in the w-correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssa.plot_w_correlation(U, sigma, Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
